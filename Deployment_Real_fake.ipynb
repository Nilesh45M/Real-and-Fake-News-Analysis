{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfdc952c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "#from xgboost import XGBClassifier\n",
    "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.metrics import f1_score,precision_score,confusion_matrix,recall_score,accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20dd829d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 11067: expected 4 fields, saw 8\\n'\n"
     ]
    }
   ],
   "source": [
    "df_fake= pd.read_csv('Fake.csv',  error_bad_lines=False, encoding= \"latin-1\")\n",
    "df_true= pd.read_csv('True.csv', error_bad_lines=False, encoding= \"latin-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e713e454",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_true['news_class'] = 1\n",
    "df_fake['news_class'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72b262e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>news_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>As U.S. budget fight looms, Republicans flip t...</td>\n",
       "      <td>WASHINGTON (Reuters) - The head of a conservat...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U.S. military to accept transgender recruits o...</td>\n",
       "      <td>WASHINGTON (Reuters) - Transgender people will...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 29, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Senior U.S. Republican senator: 'Let Mr. Muell...</td>\n",
       "      <td>WASHINGTON (Reuters) - The special counsel inv...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FBI Russia probe helped by Australian diplomat...</td>\n",
       "      <td>WASHINGTON (Reuters) - Trump campaign adviser ...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 30, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Trump wants Postal Service to charge 'much mor...</td>\n",
       "      <td>SEATTLE/WASHINGTON (Reuters) - President Donal...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 29, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  As U.S. budget fight looms, Republicans flip t...   \n",
       "1  U.S. military to accept transgender recruits o...   \n",
       "2  Senior U.S. Republican senator: 'Let Mr. Muell...   \n",
       "3  FBI Russia probe helped by Australian diplomat...   \n",
       "4  Trump wants Postal Service to charge 'much mor...   \n",
       "\n",
       "                                                text       subject  \\\n",
       "0  WASHINGTON (Reuters) - The head of a conservat...  politicsNews   \n",
       "1  WASHINGTON (Reuters) - Transgender people will...  politicsNews   \n",
       "2  WASHINGTON (Reuters) - The special counsel inv...  politicsNews   \n",
       "3  WASHINGTON (Reuters) - Trump campaign adviser ...  politicsNews   \n",
       "4  SEATTLE/WASHINGTON (Reuters) - President Donal...  politicsNews   \n",
       "\n",
       "                 date  news_class  \n",
       "0  December 31, 2017            1  \n",
       "1  December 29, 2017            1  \n",
       "2  December 31, 2017            1  \n",
       "3  December 30, 2017            1  \n",
       "4  December 29, 2017            1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_true.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28f21ff7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>news_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donald Trump Sends Out Embarrassing New Year\u0019...</td>\n",
       "      <td>Donald Trump just couldn t wish all Americans ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
       "      <td>House Intelligence Committee Chairman Devin Nu...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sheriff David Clarke Becomes An Internet Joke...</td>\n",
       "      <td>On Friday, it was revealed that former Milwauk...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 30, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trump Is So Obsessed He Even Has Obama\u0019s Name...</td>\n",
       "      <td>On Christmas day, Donald Trump announced that ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 29, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pope Francis Just Called Out Donald Trump Dur...</td>\n",
       "      <td>Pope Francis used his annual Christmas Day mes...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 25, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0   Donald Trump Sends Out Embarrassing New Year\u0019...   \n",
       "1   Drunk Bragging Trump Staffer Started Russian ...   \n",
       "2   Sheriff David Clarke Becomes An Internet Joke...   \n",
       "3   Trump Is So Obsessed He Even Has Obama\u0019s Name...   \n",
       "4   Pope Francis Just Called Out Donald Trump Dur...   \n",
       "\n",
       "                                                text subject  \\\n",
       "0  Donald Trump just couldn t wish all Americans ...    News   \n",
       "1  House Intelligence Committee Chairman Devin Nu...    News   \n",
       "2  On Friday, it was revealed that former Milwauk...    News   \n",
       "3  On Christmas day, Donald Trump announced that ...    News   \n",
       "4  Pope Francis used his annual Christmas Day mes...    News   \n",
       "\n",
       "                date  news_class  \n",
       "0  December 31, 2017           0  \n",
       "1  December 31, 2017           0  \n",
       "2  December 30, 2017           0  \n",
       "3  December 29, 2017           0  \n",
       "4  December 25, 2017           0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fake.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ec65293",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def cleanResume(resumeText):#resumText perform several operations to clean and preprocess the text of resume.\n",
    "    \n",
    "    resumeText = re.sub(r'[\\x1d\\x1c\\x1b]', '', resumeText)#remove special character\n",
    "    resumeText = re.sub(r'\\x19', ' ', resumeText) #to replace character \\x19 with space ' '.\n",
    "    resumeText = re.sub(r'\\s+', ' ', resumeText).strip()#remove whitespace\n",
    "    resumeText = re.sub('http\\S+\\s*', ' ', resumeText)  # remove URLs\n",
    "    #resumeText = re.sub('#\\S+', '', resumeText)  # remove hashtags\n",
    "    #resumeText = re.sub('@\\S+', '  ', resumeText)  # remove mentions\n",
    "    resumeText = re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\"\"), ' ', resumeText)  # remove punctuations\n",
    "    resumeText = re.sub(r'\\d+', '', resumeText)#remove number \n",
    "    resumeText = re.sub('\\s+', ' ', resumeText)  # remove extra whitespace\n",
    "    return resumeText.lower()#returns clean \"resumeText\" and convert all text into lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e76fff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_true[\"text\"]=df_true.text.apply(cleanResume)\n",
    "df_fake[\"text\"]=df_fake.text.apply(cleanResume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc37605a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "df_true[\"text\"] = df_true.text.apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b387bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fake[\"text\"]= df_fake.text.apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b57a98fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing Stop words \n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc05b8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_true[\"text\"] = df_true.text.apply(lambda x: [word for word in x if word.lower() not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31f03203",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fake[\"text\"] = df_fake.text.apply(lambda x: [word for word in x if word.lower() not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d7f2717",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "729ded5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "df_true[\"text\"] = df_true.text.apply(lambda x: [lemmatizer.lemmatize(word) for word in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99f13856",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fake[\"text\"] = df_fake.text.apply(lambda x: [lemmatizer.lemmatize(word) for word in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "68d9901d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_true['corpus_t'] = df_true['text'].apply(lambda x: ' '.join(x))\n",
    "corpus_t = df_true['corpus_t'].tolist()  # Convert the corpus_t column to a list\n",
    "df_fake['corpus_f'] = df_fake['text'].apply(lambda x: ' '.join(x))\n",
    "corpus_f = df_fake['corpus_f'].tolist()  # Convert the corpus_f column to a list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2de73194",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import ngrams\n",
    "\n",
    "def get_ngrams(corpus, n):\n",
    "    ngrams_list = []\n",
    "    for text in corpus:\n",
    "        tokens = word_tokenize(text)  # Tokenize the text into words\n",
    "        ngrams_list.extend(list(ngrams(tokens, n)))  # Generate n-grams\n",
    "    return ngrams_list\n",
    "\n",
    "def get_top_ngrams(corpus_t, corpus_f, n, top_k=100):\n",
    "    # Get n-grams for corpus_t\n",
    "    ngrams_t = Counter(get_ngrams(corpus_t, n))\n",
    "\n",
    "    # Get n-grams for corpus_f\n",
    "    ngrams_f = Counter(get_ngrams(corpus_f, n))\n",
    "\n",
    "    # Get top k frequent common n-grams\n",
    "    common_ngrams = set(ngrams_t.keys()) & set(ngrams_f.keys())\n",
    "    top_common_ngrams = sorted(common_ngrams, key=lambda x: ngrams_t[x] + ngrams_f[x], reverse=True)[:top_k]\n",
    "\n",
    "    # Get top k unique n-grams for corpus_t\n",
    "    unique_ngrams_t = set(ngrams_t.keys()) - set(ngrams_f.keys())\n",
    "    top_unique_ngrams_t = sorted(unique_ngrams_t, key=lambda x: ngrams_t[x], reverse=True)[:top_k]\n",
    "\n",
    "    # Get top k unique n-grams for corpus_f\n",
    "    unique_ngrams_f = set(ngrams_f.keys()) - set(ngrams_t.keys())\n",
    "    top_unique_ngrams_f = sorted(unique_ngrams_f, key=lambda x: ngrams_f[x], reverse=True)[:top_k]\n",
    "\n",
    "    return top_common_ngrams, top_unique_ngrams_t, top_unique_ngrams_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8338977a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unigrams\n",
    "n =1 \n",
    "top_common_unigrams, top_unique_unigrams_t, top_unique_unigrams_f = get_top_ngrams(corpus_t, corpus_f, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f20d54ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bigrams\n",
    "n = 2\n",
    "top_common_bigrams, top_unique_bigrams_t, top_unique_bigrams_f = get_top_ngrams(corpus_t, corpus_f, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "87376f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trigrams\n",
    "n = 3\n",
    "top_common_trigrams, top_unique_trigrams_t, top_unique_trigrams_f = get_top_ngrams(corpus_t, corpus_f, n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4bcead33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_ngrams(corpus, ngrams_list):\n",
    "    for ngram in ngrams_list:\n",
    "        while ngram in corpus:\n",
    "            corpus.remove(ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "355ca02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_ngrams(corpus_t, top_common_unigrams)\n",
    "remove_ngrams(corpus_f, top_common_unigrams)\n",
    "remove_ngrams(corpus_t, top_common_bigrams)\n",
    "remove_ngrams(corpus_f, top_common_bigrams)\n",
    "remove_ngrams(corpus_t, top_common_trigrams)\n",
    "remove_ngrams(corpus_f, top_common_trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f012dfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_stopwords=[\n",
    "'according', 'administration','also', 'america', 'american', 'americans', 'another', 'back', 'bill', 'black', 'called', 'campaign', 'clinton', 'could', 'country', 'day', 'department', 'donald', 'election', 'even', 'every', 'fact', 'first', 'former', 'fox', 'get', 'go', 'going', 'good', 'government', 'group', 'hillary', 'house', 'Image', 'it', 'know', 'last', 'law', 'like', 'made', 'make', 'may', 'media', 'much', 'national', 'never', 'new', 'news', 'man', 'many', 'obama', 'office', 'one', 'party', 'people', 'police', 'political', 'president', 'presidential', 'public', 'really', 'republican', 'republicans', 'right', 'said', 'say', 'says', 'see', 'show', 'since', 'state', 'states', 'still', 'support', 'take', 'think', 'time', 'told', 'trump', 'two', 'united', 'us', 'via', 'video', 'vote', 'white', 'women', 'world', 'would', 'year', 'years', 'want'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6ea44682",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_true[\"text\"] = df_true.text.apply(lambda x: [word for word in x if word.lower() not in custom_stopwords])\n",
    "df_fake[\"text\"] = df_fake.text.apply(lambda x: [word for word in x if word.lower() not in custom_stopwords])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eb0de9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_t = df_true['text'].apply(lambda x: ' '.join(x)).tolist()\n",
    "corpus_f = df_fake['text'].apply(lambda x: ' '.join(x)).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "46e868d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = corpus_t + corpus_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1b4b5153",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [0] * len(corpus_f) + [1] * len(corpus_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "92150fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 3), max_features=1000)\n",
    "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "tfidf_array = tfidf_matrix.toarray()\n",
    "feature_names = vectorizer.get_feature_names()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "29eecb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest Classifier\n",
    "rf = RandomForestClassifier()\n",
    "x_train, x_test, y_train, y_test = train_test_split(tfidf_array, labels, test_size=0.2, random_state=42)\n",
    "rf.fit(x_train,y_train)\n",
    "y_pred_rf = rf.predict(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c3c6c7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create a pickle file using serialization \n",
    "import pickle\n",
    "pickle_out = open(\"rf.pkl\",\"wb\")\n",
    "pickle.dump(rf,pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "636b9507",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tfidf_vectorizer.pkl', 'wb') as f:\n",
    "    pickle.dump(vectorizer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "82871bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ee6f2976",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_in = open(\"rf.pkl\",\"rb\")\n",
    "rf = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6730aea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f9c0826a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the trained model\n",
    "model = joblib.load('rf.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "887b04d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-08 14:10:16.935 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run C:\\Users\\Dell\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n"
     ]
    }
   ],
   "source": [
    "# Define the Streamlit App\n",
    "def main():\n",
    "    # Add title and description\n",
    "    st.title(\"Text Classification Demo\")\n",
    "    st.write(\"Enter the text and click on the 'Predict' button in order to classify if it is Fake or True.\")\n",
    "\n",
    "    # Create a text input box for user input\n",
    "    text_input = st.text_area(\"Enter Text\",\"\")\n",
    "\n",
    "    # Add a predict button\n",
    "    if st.button(\"Predict\"):\n",
    "        # Preprocess the input text\n",
    "        preprocessed_text = preprocessed_text(text_input)\n",
    "        # Make a prediction using the loaded model\n",
    "        prediction = model.predict([preprocessed_text])[0]\n",
    "        # Map prediction to 'Fake or True'\n",
    "        prediction_label = \"Fake\" if prediction == 0 else \"True\"\n",
    "        # Display the prediction\n",
    "        st.subheader(\"Prediction\")\n",
    "        st.write(prediction_label)\n",
    "\n",
    "# Run the app\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7646a308",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
